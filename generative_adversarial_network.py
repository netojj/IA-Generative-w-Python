# -*- coding: utf-8 -*-
"""Generative Adversarial Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aBHuDITyCK0S_iq2bhKdwmju0w-eZdAK
"""

import torch
import torch.nn as nn #Permite criar classe pra criar rede neural, tanto do discriminador, quanto do gerador
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torchvision.utils import save_image
import matplotlib.pyplot as plt
import numpy as np
import random

mnist_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())

indices = random.sample(range(100), 9) #Gera 9 índices aleatórios
images = [mnist_data[i][0] for i in indices] #Seleciona os índices no mnist

fig, axes = plt.subplots(3, 3, figsize=(2, 2)) #Cria dois objetos a partir do subplots através do matplotlib para criar uma grade, figsize define tamanho

for i, ax in enumerate(axes.flat):
    ax.imshow(images[i].squeeze().numpy(), cmap='gray')
    ax.axis('off')

plt.show()

# Hipe parametros
batch_size = 128 #tamanho do lote
latent_dim = 100 #tamanho da dimensão latente
lr = 0.0002 #learning rate
epochs = 100 #qts vezes os dados vão passar na rede neural
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#pré-processamento
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

dataloader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=True,
                    download=True, transform=transform), batch_size=batch_size, shuffle=True)

# Generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, 28*28),
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z).view(z.size(0), 1, 28, 28)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(28*28, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        return self.model(img_flat)

generator = Generator().to(device)

discriminator = Discriminator().to(device)

optimizer_G = optim.Adam(generator.parameters(), lr=lr)

optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)

criterion = nn.BCELoss()

for epoch in range(epochs):
    d_loss_accumulated = 0.0
    g_loss_accumulated = 0.0

    for i, (imgs, _) in enumerate(dataloader):
        real = torch.ones(imgs.size(0), 1).to(device)
        fake = torch.zeros(imgs.size(0), 1).to(device)

        # Treinamento do Discriminator
        optimizer_D.zero_grad()
        real_imgs = imgs.to(device)
        pred_real = discriminator(real_imgs)
        loss_real = criterion(pred_real, real)

        z = torch.randn(imgs.size(0), latent_dim).to(device)
        fake_imgs = generator(z)
        pred_fake = discriminator(fake_imgs.detach())
        loss_fake = criterion(pred_fake, fake)

        d_loss = (loss_real + loss_fake) / 2
        d_loss.backward()
        optimizer_D.step()

        d_loss_accumulated += d_loss.item()

        # Treinamento do Generator
        optimizer_G.zero_grad()
        pred_fake = discriminator(fake_imgs)
        g_loss = criterion(pred_fake, real)
        g_loss.backward()
        optimizer_G.step()

        # Acumula a perda do Generator
        g_loss_accumulated += g_loss.item()

        # Se estiver no último batch da época e a época é um múltiplo de 10
        if i == len(dataloader) - 1 and (epoch + 1) % 10 == 0:
            d_loss_avg = d_loss_accumulated / len(dataloader)
            g_loss_avg = g_loss_accumulated / len(dataloader)

            print(f"[Epoch {epoch+1}/{epochs}] [Avg D loss: {d_loss_avg}] [Avg G loss: {g_loss_avg}]")

            grid = torchvision.utils.make_grid(fake_imgs.data[:9], nrow=3, normalize=True)
            grid_numpy = grid.permute(1, 2, 0).cpu().numpy()

            plt.figure(figsize=(2, 2))
            plt.imshow(grid_numpy)
            plt.axis('off')
            plt.show()